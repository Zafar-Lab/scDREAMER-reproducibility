{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3c2017-e427-402b-a767-c9223fd19e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63901383-d1e4-41f9-98db-e3c3ac0e3a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "import scipy.sparse\n",
    "from sklearn.metrics.cluster import normalized_mutual_info_score as nmi\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from scipy import stats \n",
    "from scipy import * \n",
    "import datetime \n",
    "import pandas as pd\n",
    "import scanpy as sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc13323f-cc06-4035-b7bb-fc1be10d2028",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "tf.set_random_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9796f1-0810-45a2-9165-3e61ca89c12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class batch_norm(object):\n",
    "  def __init__(self, epsilon=1e-5, momentum = 0.9, name=\"batch_norm\"):\n",
    "    with tf.variable_scope(name):\n",
    "      self.epsilon  = epsilon\n",
    "      self.momentum = momentum\n",
    "      self.name = name\n",
    "\n",
    "  def __call__(self, x, train=True):\n",
    "    return tf.contrib.layers.batch_norm(x,\n",
    "                      decay=self.momentum,\n",
    "                      updates_collections=None,\n",
    "                      epsilon=self.epsilon,\n",
    "                      scale=True,\n",
    "                      is_training=train,\n",
    "                      scope=self.name)\n",
    "\n",
    "\n",
    "# Leaky Relu\n",
    "def lrelu(x, alpha = 0.2, name='lrelu'):\n",
    "    return tf.maximum(x, alpha*x)\n",
    "\n",
    "def dense(x, inp_dim, out_dim, name = 'dense'):\n",
    "    \"\"\"\n",
    "    Used to create a dense layer.\n",
    "    :param x: input tensor to the dense layer\n",
    "    :param inp_dim: no. of input neurons\n",
    "    :param out_dim: no. of output neurons\n",
    "    :param name: name of the entire dense layer.i.e, variable scope name.\n",
    "    :return: tensor with shape [batch_size, out_dim]\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(name, reuse=None):\n",
    "        weights = tf.get_variable(\"weights\", shape=[inp_dim, out_dim],\n",
    "                                  initializer=tf.contrib.layers.xavier_initializer())\n",
    "        bias = tf.get_variable(\"bias\", shape=[out_dim], initializer=tf.constant_initializer(0.0))\n",
    "        out = tf.add(tf.matmul(x, weights), bias, name='matmul')\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5421f77d-efe2-42bc-9c8e-24bbd58a180f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "import scanpy as sc\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n",
    "def load_data(dataset_path,batch_key, cell_type_key, x_dim = 2000):\n",
    "    Ann = sc.read_h5ad(dataset_path)\n",
    "    Ann.layers[\"counts\"] = Ann.X.copy()\n",
    "    sc.pp.normalize_total(Ann, target_sum=1e4)\n",
    "    sc.pp.log1p(Ann)\n",
    "    Ann.raw = Ann\n",
    "    sc.pp.highly_variable_genes(\n",
    "            Ann,\n",
    "            flavor=\"seurat\",\n",
    "            n_top_genes=x_dim,\n",
    "            layer=\"counts\",\n",
    "            batch_key=batch_key,\n",
    "            subset=True)\n",
    "    if type(Ann.X) != type(np.array([])):\n",
    "        data = Ann.X.toarray()\n",
    "    else:\n",
    "        data = Ann.X\n",
    "    print('please see', data.shape)\n",
    "    print('Data Loaded from {} !'.format(dataset_path))\n",
    "    total_size = data.shape[0]\n",
    "\n",
    "    batch_data = np.array(Ann.obs[batch_key].astype(\"category\")).reshape(-1,1)\n",
    "    enc = OneHotEncoder()\n",
    "    batch_data_enc = enc.fit_transform(batch_data).toarray()\n",
    "\n",
    "    cell_type_data = Ann.obs[cell_type_key].to_numpy()\n",
    "\n",
    "    np.random.seed(0)\n",
    "\n",
    "    indx = np.random.permutation(np.arange(total_size))\n",
    "    data = data[indx, :]\n",
    "    batch_data_enc = batch_data_enc[indx,:]\n",
    "    batch_data = batch_data[indx]\n",
    "    cell_type_data = cell_type_data[indx]\n",
    "\n",
    "    return data,batch_data_enc, batch_data, cell_type_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13be475-ae87-48b5-8be7-d5d219c83d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "# from opts import *\n",
    "# import Util\n",
    "import numpy as np\n",
    "import scipy.sparse\n",
    "from sklearn.metrics.cluster import normalized_mutual_info_score as nmi\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from scipy import stats \n",
    "from scipy import * \n",
    "import datetime "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788094a8-a9e0-46bb-9336-cc979dc2b763",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Test_DRA(object):\n",
    "    def __init__(self, sess, dataset_path, batch_key, cell_type_key, epoch=200, lr=0.0001, beta1=0.5, batch_size=128, X_dim = 2000, z_dim=10,\n",
    "                 checkpoint_dir='checkpoint', sample_dir='samples', result_dir='result', num_layers=2, g_h_dim=None,\n",
    "                 d_h_dim=None, gen_activation='sig', leak=0.2, keep_param=1.0, trans='sparse', is_bn=False,\n",
    "                 g_iter=2, lam=10.0, sampler='uniform'):\n",
    "\n",
    "        self.sess = sess\n",
    "        self.dataset_path = dataset_path\n",
    "        self.dataset_name = dataset_path.split('/')[-1]\n",
    "        self.batch_key = batch_key\n",
    "        self.cell_type_key = cell_type_key\n",
    "        self.epoch = epoch\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.batch_size = batch_size\n",
    "        self.X_dim = X_dim\n",
    "        self.z_dim = z_dim\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "        self.sample_dir = sample_dir\n",
    "        self.result_dir = result_dir\n",
    "        self.num_layers = num_layers\n",
    "        self.g_h_dim = g_h_dim  # Fully connected layers for Generator\n",
    "        self.d_h_dim = d_h_dim  # Fully connected layers for Discriminator\n",
    "        self.gen_activation = gen_activation\n",
    "        self.leak = leak\n",
    "        self.keep_param = keep_param\n",
    "        self.trans = trans\n",
    "        self.is_bn = is_bn\n",
    "        self.g_iter = g_iter\n",
    "        self.lam = lam\n",
    "        self.sampler = sampler\n",
    "        self.eps = 0.001\n",
    "        self._is_train = False\n",
    "        self.n_hidden = 128\n",
    "\n",
    "        self.data, self.batch_data_enc, self.batch_data_non_enc, self.cell_type_data = load_data(self.dataset_path, self.batch_key, self.cell_type_key, self.X_dim )\n",
    "        self.N_batch = self.batch_data_enc.shape[1]\n",
    "        print('N_batch......',self.N_batch)\n",
    "        print('number of cell_types',np.unique(self.cell_type_data))\n",
    "\n",
    "\n",
    "        if self.gen_activation == 'tanh':\n",
    "            self.data = 2 * self.data - 1\n",
    "            # self.data_train = 2 * self.data_train - 1\n",
    "            # self.data_val = 2 * self.data_val - 1\n",
    "            # self.data_test = 2 * self.data_test - 1\n",
    "\n",
    "        self.total_size = self.data.shape[0]\n",
    "      \n",
    "\n",
    "        self.build_model()\n",
    "\n",
    "    def build_model(self):\n",
    "\n",
    "        self.x_input = tf.placeholder(dtype=tf.float32, shape=[None, self.X_dim], name='Input')\n",
    "        self.x_target = tf.placeholder(dtype=tf.float32, shape=[None, self.X_dim], name='Target')\n",
    "        self.keep_prob = tf.placeholder(dtype=tf.float32, name = 'keep_prob')\n",
    "        self.real_distribution = tf.placeholder(dtype=tf.float32, shape=[None, self.z_dim], name='Real_distribution')\n",
    "        self.kl_scale = tf.placeholder(tf.float32, (), name='kl_scale')\n",
    "        \n",
    "        self.kl_scale = 0        \n",
    "        self.dropout_rate = 0.1 \n",
    "        self.training_phase = True \n",
    "        self.n_layers = self.num_layers \n",
    "        self.n_latent = self.z_dim\n",
    "        \n",
    "        self.encoder_output, self.z_post_m, self.z_post_v, self.l_post_m, self.l_post_v = self.encoder(self.x_input) \n",
    "        self.expression = self.x_input               \n",
    "        self.proj = tf.placeholder(dtype=tf.float32, shape=[None, self.X_dim], name='projection')\n",
    "      \n",
    "        log_library_size = np.log(np.sum(self.data, axis=1)) \n",
    "        mean, variance = np.mean(log_library_size), np.var(log_library_size)\n",
    "        library_size_mean = mean\n",
    "        library_size_variance = variance\n",
    "        self.library_size_mean = tf.to_float(tf.constant(library_size_mean))\n",
    "        self.library_size_variance = tf.to_float(tf.constant(library_size_variance))        \n",
    "        self.z = self.sample_gaussian(self.z_post_m, self.z_post_v) \n",
    "        self.library = self.sample_gaussian(self.l_post_m, self.l_post_v)\n",
    "        self.decoder_output = self.decoder(self.z)               \n",
    "        self.n_input = self.expression.get_shape().as_list()[1] \n",
    "      \n",
    "        self.x_post_scale = tf.nn.softmax(dense(self.decoder_output, self.g_h_dim[0], self.n_input, name='dec_x_post_scale')) \n",
    "        self.x_post_r = tf.Variable(tf.random_normal([self.n_input]), name=\"dec_x_post_r\")           \n",
    "        self.x_post_rate = tf.exp(self.library) * self.x_post_scale\n",
    "        self.x_post_dropout = dense(self.decoder_output, self.g_h_dim[0], self.n_input, name='dec_x_post_dropout') \n",
    "            \n",
    "        local_dispersion = tf.exp(self.x_post_r)            \n",
    "        local_l_mean = self.library_size_mean\n",
    "        local_l_variance = self.library_size_variance\n",
    "\n",
    "        self.decoder_output2 = tf.nn.sigmoid(dense(self.decoder_output, self.g_h_dim[0], self.X_dim, 'dec_output2'))        \n",
    "        self.dis_real_logit = self.discriminator(self.real_distribution, self.z_dim) \n",
    "        self.dis_fake_logit = self.discriminator(self.z, self.z_dim, reuse=True) \n",
    "       \n",
    "        # Discriminator D2\n",
    "        self.dis2_real_logit = self.discriminator2(self.x_target, self.X_dim)       \n",
    "        self.dis2_fake_logit = self.discriminator2(self.decoder_output2, self.X_dim, reuse=True)         \n",
    "        \n",
    "        # Reconstruction loss \n",
    "        recon_loss = self.zinb_model(self.expression, self.x_post_rate, local_dispersion, self.x_post_dropout)        \n",
    "        \n",
    "        kl_gauss_l = 0.5 * tf.reduce_sum(- tf.log(self.l_post_v + 1e-8)  \\\n",
    "                                         + self.l_post_v/local_l_variance \\\n",
    "                                         + tf.square(self.l_post_m - local_l_mean)/local_l_variance  \\\n",
    "                                         + tf.log(local_l_variance + 1e-8) - 1, 1)\n",
    "\n",
    "        kl_gauss_z = 0.5 * tf.reduce_sum(- tf.log(self.z_post_v + 1e-8) + self.z_post_v + tf.square(self.z_post_m) - 1, 1)\n",
    "        \n",
    "        # Evidence lower bound        \n",
    "        self.ELBO_gauss = tf.reduce_mean(recon_loss - kl_gauss_l - self.kl_scale * kl_gauss_z) \n",
    "        self.autoencoder_loss = - self.ELBO_gauss                                \n",
    "              \n",
    "        # Discriminator D1        \n",
    "        self.dis_loss = - tf.log(tf.reduce_sum(tf.sqrt(tf.abs(self.dis_real_logit/tf.reduce_sum(self.dis_real_logit)\n",
    "                                            * self.dis_fake_logit/tf.reduce_sum(self.dis_fake_logit)) )) + 1e-10)        \n",
    "                \n",
    "        # Discriminator D2 \n",
    "        self.dis2_loss = - tf.log(tf.reduce_sum(tf.sqrt(tf.abs(self.dis2_real_logit/tf.reduce_sum(self.dis2_real_logit)\n",
    "                                            * self.dis2_fake_logit/tf.reduce_sum(self.dis2_fake_logit)) )) + 1e-10)\n",
    "        \n",
    "        # Generator loss\n",
    "        self.generator_loss = - tf.log(tf.reduce_sum(tf.sqrt(tf.abs(\n",
    "                                            self.dis_fake_logit/tf.reduce_sum(self.dis_fake_logit)))) + 1e-10)\n",
    "                \n",
    "        t_vars = tf.trainable_variables()\n",
    "        self.dis_vars = [var for var in t_vars if 'dis_' in var.name]\n",
    "        self.gen_vars = [var for var in t_vars if 'enc_' in var.name]\n",
    "\n",
    "        # Discriminator D2\n",
    "        self.dis2_vars = [var for var in t_vars if 'dis2_' in var.name]\n",
    "\n",
    "        self.saver = tf.train.Saver()\n",
    "\n",
    "    def train_cluster(self):\n",
    "\n",
    "        print('Cluster DRA on DataSet {} ... '.format(self.dataset_name))\n",
    "\n",
    "        autoencoder_optimizer = tf.train.AdamOptimizer(learning_rate=self.lr,\n",
    "                                                       beta1=self.beta1).minimize(self.autoencoder_loss)\n",
    "        \n",
    "        discriminator_optimizer = tf.train.AdamOptimizer(learning_rate=self.lr,\n",
    "                                                         beta1=self.beta1).minimize(self.dis_loss,\n",
    "                                                                                      var_list=self.dis_vars)\n",
    "        generator_optimizer = tf.train.AdamOptimizer(learning_rate=self.lr,\n",
    "                                                     beta1=self.beta1).minimize(self.generator_loss,\n",
    "                                                                                  var_list=self.gen_vars)\n",
    "        # Discriminator D2\n",
    "        discriminator2_optimizer = tf.train.AdamOptimizer(learning_rate=self.lr,\n",
    "                                                         beta1=self.beta1).minimize(self.dis2_loss,\n",
    "                                                                                      var_list=self.dis2_vars)\n",
    "\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        a_loss_epoch = []\n",
    "        d_loss_epoch = []\n",
    "        g_loss_epoch = []\n",
    "        d2_loss_epoch = [] # Discriminator D2\n",
    "\n",
    "        control = 3 # Generator is updated twice for each Discriminator D1 update\n",
    "\n",
    "        num_batch_iter = self.total_size // self.batch_size\n",
    "        for ep in range(self.epoch):\n",
    "            self.ep = ep\n",
    "            d_loss_curr = g_loss_curr = a_loss_curr = np.inf\n",
    "            self._is_train = True\n",
    "            for it in range(num_batch_iter):\n",
    "\n",
    "                batch_x = self.next_batch(self.data, self.total_size)                \n",
    "                batch_z_real_dist = self.sample_Z(self.batch_size, self.z_dim)\n",
    "\n",
    "                _, a_loss_curr = self.sess.run([autoencoder_optimizer, self.autoencoder_loss],\n",
    "                                               feed_dict={self.x_input: batch_x, self.x_target: batch_x,\n",
    "                                                        self.keep_prob: self.keep_param}) \n",
    "\n",
    "                if np.mod(it, control) == 0: \n",
    "                    \n",
    "                    _, d_loss_curr = self.sess.run([discriminator_optimizer, self.dis_loss],\n",
    "                        feed_dict={self.x_input: batch_x,\n",
    "                        self.real_distribution: batch_z_real_dist,\n",
    "                        self.keep_prob: self.keep_param})                     \n",
    "                    \n",
    "                else: \n",
    "                    \n",
    "                    _, g_loss_curr = self.sess.run([generator_optimizer, self.generator_loss],\n",
    "                        feed_dict={self.x_input: batch_x, self.keep_prob: self.keep_param}) \n",
    "\n",
    "                    \n",
    "                _, d2_loss_curr = self.sess.run([discriminator2_optimizer, self.dis2_loss],\n",
    "                        feed_dict={self.x_input: batch_x,\n",
    "                        self.x_target: batch_x,\n",
    "                        self.keep_prob: self.keep_param}) \n",
    "                                    \n",
    "            self._is_train = False\n",
    "            a_loss_epoch.append(a_loss_curr)\n",
    "            d_loss_epoch.append(d_loss_curr)\n",
    "            g_loss_epoch.append(g_loss_curr)\n",
    "            d2_loss_epoch.append(d2_loss_curr)\n",
    "            \n",
    "            print(                \n",
    "            \"Epoch : [%d] ,  a_loss = %.4f, d_loss: %.4f ,  g_loss: %.4f, d2_loss: %.4f\"\n",
    "                                        % (ep, a_loss_curr, d_loss_curr, g_loss_curr, d2_loss_curr))    \n",
    "       \n",
    "        self.eval_cluster_on_test()\n",
    "\n",
    "    # The autoencoder network\n",
    "    def encoder(self, x, reuse=False):\n",
    "        \"\"\"\n",
    "        Encode part of the autoencoder.\n",
    "        :param x: input to the autoencoder\n",
    "        :param reuse: True -> Reuse the encoder variables, False -> Create or search of variables before creating\n",
    "        :return: tensor which is the hidden latent variable of the autoencoder.\n",
    "        \"\"\"\n",
    "\n",
    "        with tf.variable_scope('Encoder') as scope:\n",
    "            if reuse:\n",
    "                scope.reuse_variables()\n",
    "\n",
    "            if self.is_bn:\n",
    "                h = tf.layers.batch_normalization(\n",
    "\n",
    "                    lrelu(dense(x, self.X_dim, self.g_h_dim[0], name='enc_h0_lin'), alpha=self.leak),\n",
    "                    training=self._is_train, name='enc_bn0')\n",
    "                    \n",
    "                for i in range(1, self.num_layers):\n",
    "                    h = tf.layers.batch_normalization(\n",
    "\n",
    "                        lrelu(dense(h, self.g_h_dim[i - 1], self.g_h_dim[i], name='enc_h' + str(i) + '_lin'),\n",
    "                              alpha=self.leak),\n",
    "                        training=self._is_train, name='enc_bn' + str(i))                    \n",
    "\n",
    "                z_post_m = dense(h, self.g_h_dim[self.num_layers - 1], self.z_dim, name='enc_z_post_m' + str(self.num_layers) + '_lin')                \n",
    "                z_post_v = tf.exp(dense(h, self.g_h_dim[self.num_layers - 1], self.z_dim, name='enc_z_post_v' + str(self.num_layers) + '_lin'))              \n",
    "                \n",
    "                h = tf.nn.relu(dense(h, self.g_h_dim[self.num_layers - 1], self.z_dim, name='enc_h' + str(self.num_layers) + '_lin'))\n",
    "\n",
    "                l_post_m = dense(h, self.z_dim, 1, name='enc_l_post_m' + str(self.num_layers) + '_lin')                            \n",
    "                l_post_v = tf.exp(dense(h, self.z_dim, 1, name='enc_l_post_v' + str(self.num_layers) + '_lin')) \n",
    "                \n",
    "\n",
    "            else:\n",
    "\n",
    "                h = tf.nn.dropout(lrelu(dense(x, self.X_dim, self.g_h_dim[0], name='enc_h0_lin'), alpha=self.leak),\n",
    "                                  keep_prob=self.keep_prob)                \n",
    "                \n",
    "                for i in range(1, self.num_layers):\n",
    "                    \n",
    "                    h = tf.nn.dropout(lrelu(dense(h, self.g_h_dim[i - 1], self.g_h_dim[i], name='enc_h' + str(i) + '_lin'),\n",
    "                              alpha=self.leak), keep_prob=self.keep_prob)                    \n",
    "\n",
    "                z_post_m = dense(h, self.g_h_dim[self.num_layers - 1], self.z_dim, name='enc_z_post_m' + str(self.num_layers) + '_lin')                \n",
    "                z_post_v = tf.exp(dense(h, self.g_h_dim[self.num_layers - 1], self.z_dim, name='enc_z_post_v' + str(self.num_layers) + '_lin'))\n",
    "                \n",
    "                \n",
    "                h = tf.nn.relu(dense(h, self.g_h_dim[self.num_layers - 1], self.z_dim, name='enc_h' + str(self.num_layers) + '_lin'))\n",
    "                          \n",
    "\n",
    "                l_post_m = dense(h, self.z_dim, 1, name='enc_l_post_m' + str(self.num_layers) + '_lin')                             \n",
    "                l_post_v = tf.exp(dense(h, self.z_dim, 1, name='enc_l_post_v' + str(self.num_layers) + '_lin'))                                          \n",
    "                            \n",
    "            return h, z_post_m, z_post_v, l_post_m, l_post_v\n",
    "\n",
    "\n",
    "    def decoder(self, z, reuse=False):\n",
    "        \"\"\"\n",
    "        Decoder part of the autoencoder.\n",
    "        :param z: input to the decoder\n",
    "        :param reuse: True -> Reuse the decoder variables, False -> Create or search of variables before creating\n",
    "        :return: tensor which should ideally be the input given to the encoder.\n",
    "        \"\"\"\n",
    "\n",
    "        with tf.variable_scope('Decoder') as scope:\n",
    "            if reuse:\n",
    "                scope.reuse_variables()\n",
    "\n",
    "            if self.is_bn:\n",
    "\n",
    "                h = tf.layers.batch_normalization(\n",
    "                  \n",
    "                    lrelu(dense(z, self.z_dim, self.g_h_dim[self.num_layers-1], name='dec_h' + str(self.num_layers-1) + '_lin'),\n",
    "                          alpha=self.leak),                   \n",
    "                    training=self._is_train, name='dec_bn' + str(self.num_layers-1))\n",
    "                for i in range(self.num_layers-2, -1,-1):\n",
    "                    h = tf.layers.batch_normalization(\n",
    "\n",
    "                        lrelu(dense(h, self.g_h_dim[i + 1], self.g_h_dim[i], name='dec_h' + str(i) + '_lin'),\n",
    "                             alpha=self.leak),                        \n",
    "                        training=self._is_train, name='dec_bn' + str(i))\n",
    "            else:\n",
    "                h = tf.nn.dropout(lrelu(dense(z, self.z_dim, self.g_h_dim[self.num_layers-1], name='dec_h' + str(self.num_layers-1) + '_lin'),\n",
    "                                        alpha=self.leak),                                  \n",
    "                                  keep_prob=self.keep_prob)\n",
    "                for i in range(self.num_layers-2, -1, -1):\n",
    "                    h = tf.nn.dropout(\n",
    "                        lrelu(dense(h, self.g_h_dim[i + 1], self.g_h_dim[i], name='dec_h' + str(i) + '_lin'),\n",
    "                              alpha=self.leak), keep_prob=self.keep_prob)\n",
    "               \n",
    "            return h\n",
    "\n",
    "\n",
    "\n",
    "    def discriminator(self, z, z_dim, reuse=False):    \n",
    "        \"\"\"\n",
    "        Discriminator that is used to match the posterior distribution with a given prior distribution.\n",
    "        :param z: tensor of shape [batch_size, z_dim]\n",
    "        :param reuse: True -> Reuse the discriminator variables,\n",
    "                      False -> Create or search of variables before creating\n",
    "        :return: tensor of shape [batch_size, 1]\n",
    "        \"\"\"\n",
    "        with tf.variable_scope('Discriminator') as scope:\n",
    "            if reuse:\n",
    "                scope.reuse_variables()\n",
    "\n",
    "            if self.is_bn:\n",
    "\n",
    "                h = tf.layers.batch_normalization(\n",
    "                    lrelu(dense(z, z_dim, self.d_h_dim[self.num_layers - 1], name='dis_h' + str(self.num_layers-1) + '_lin'),      \n",
    "                          alpha=self.leak),\n",
    "                    training=self._is_train, name='dis_bn' + str(self.num_layers-1))\n",
    "                for i in range(self.num_layers - 2, -1, -1):\n",
    "                    h = tf.layers.batch_normalization(\n",
    "                        lrelu(dense(h, self.d_h_dim[i + 1], self.d_h_dim[i], name='dis_h' + str(i) + '_lin'),\n",
    "                              alpha=self.leak),\n",
    "                        training=self._is_train, name='dis_bn' + str(i))\n",
    "\n",
    "            else:\n",
    "\n",
    "                h = tf.nn.dropout(\n",
    "                    lrelu(dense(z, z_dim, self.d_h_dim[self.num_layers - 1], name='dis_h' + str(self.num_layers-1) + '_lin'),\n",
    "                          alpha=self.leak),\n",
    "                    keep_prob=self.keep_prob)\n",
    "                for i in range(self.num_layers - 2, -1, -1):\n",
    "                    h = tf.nn.dropout(\n",
    "                        lrelu(dense(h, self.d_h_dim[i + 1], self.d_h_dim[i], name='dis_h' + str(i) + '_lin'),\n",
    "                              alpha=self.leak), keep_prob=self.keep_prob)\n",
    "\n",
    "            output = dense(h, self.d_h_dim[0], 1, name='dis_output')\n",
    "            return output\n",
    "\n",
    "    def discriminator2(self, z, z_dim, reuse=False):    \n",
    "        \"\"\"\n",
    "        Discriminator that is used to match the posterior distribution with a given prior distribution.\n",
    "        :param z: tensor of shape [batch_size, z_dim]\n",
    "        :param reuse: True -> Reuse the discriminator variables,\n",
    "                      False -> Create or search of variables before creating\n",
    "        :return: tensor of shape [batch_size, 1]\n",
    "        \"\"\"\n",
    "        with tf.variable_scope('Discriminator2') as scope:\n",
    "            if reuse:\n",
    "                scope.reuse_variables()\n",
    "\n",
    "            if self.is_bn:\n",
    "\n",
    "                h = tf.layers.batch_normalization(\n",
    "                    lrelu(dense(z, z_dim, self.d_h_dim[self.num_layers - 1], name='dis2_h' + str(self.num_layers-1) + '_lin'),      \n",
    "                          alpha=self.leak),\n",
    "                    training=self._is_train, name='dis2_bn' + str(self.num_layers-1))\n",
    "                for i in range(self.num_layers - 2, -1, -1):\n",
    "                    h = tf.layers.batch_normalization(\n",
    "                        lrelu(dense(h, self.d_h_dim[i + 1], self.d_h_dim[i], name='dis2_h' + str(i) + '_lin'),\n",
    "                              alpha=self.leak),\n",
    "                        training=self._is_train, name='dis2_bn' + str(i))\n",
    "\n",
    "            else:\n",
    "\n",
    "                h = tf.nn.dropout(\n",
    "                    lrelu(dense(z, z_dim, self.d_h_dim[self.num_layers - 1], name='dis2_h' + str(self.num_layers-1) + '_lin'),\n",
    "                          alpha=self.leak),\n",
    "                    keep_prob=self.keep_prob)\n",
    "                for i in range(self.num_layers - 2, -1, -1):\n",
    "                    h = tf.nn.dropout(\n",
    "                        lrelu(dense(h, self.d_h_dim[i + 1], self.d_h_dim[i], name='dis2_h' + str(i) + '_lin'),\n",
    "                              alpha=self.leak), keep_prob=self.keep_prob)\n",
    "\n",
    "            output = dense(h, self.d_h_dim[0], 1, name='dis2_output')\n",
    "            return output\n",
    "\n",
    "\n",
    "    @property\n",
    "    def model_dir(self):\n",
    "        s = \"DRA_{}_{}_b_{}_g{}_d{}_{}_{}_lr_{}_b1_{}_leak_{}_keep_{}_z_{}_{}_bn_{}_lam_{}_giter_{}_epoch_{}\".format(\n",
    "            datetime.datetime.now(), self.dataset_name, \n",
    "            self.batch_size, self.g_h_dim, self.d_h_dim, self.gen_activation, self.trans, self.lr, \n",
    "            self.beta1, self.leak, self.keep_param, self.z_dim, self.sampler, self.is_bn,\n",
    "            self.lam, self.g_iter, self.epoch) \n",
    "        s = s.replace('[', '_')\n",
    "        s = s.replace(']', '_')\n",
    "        s = s.replace(' ', '')\n",
    "        return s\n",
    "\n",
    "    def sample_Z(self, m, n, sampler='uniform'):\n",
    "        if self.sampler == 'uniform':\n",
    "            return np.random.uniform(-1., 1., size=[m, n])\n",
    "        elif self.sampler == 'normal':\n",
    "            return np.random.randn(m, n)\n",
    "\n",
    "    def next_batch(self, data, max_size):\n",
    "        indx = np.random.randint(max_size - self.batch_size)\n",
    "        return data[indx:(indx + self.batch_size), :]\n",
    "\n",
    "    def sample_gaussian(self, mean, variance, scope=None):\n",
    "\n",
    "        with tf.variable_scope(scope, 'sample_gaussian'):\n",
    "            sample = tf.random_normal(tf.shape(mean), mean, tf.sqrt(variance))\n",
    "            sample.set_shape(mean.get_shape())\n",
    "            return sample\n",
    "\n",
    "    # Zero-inflated negative binomial (ZINB) model is for modeling count variables with excessive zeros and it is usually for overdispersed count outcome variables.\n",
    "    def zinb_model(self, x, mean, inverse_dispersion, logit, eps=1e-8): \n",
    "                                          \n",
    "        expr_non_zero = - tf.nn.softplus(- logit) \\\n",
    "                        + tf.log(inverse_dispersion + eps) * inverse_dispersion \\\n",
    "                        - tf.log(inverse_dispersion + mean + eps) * inverse_dispersion \\\n",
    "                        - x * tf.log(inverse_dispersion + mean + eps) \\\n",
    "                        + x * tf.log(mean + eps) \\\n",
    "                        - tf.lgamma(x + 1) \\\n",
    "                        + tf.lgamma(x + inverse_dispersion) \\\n",
    "                        - tf.lgamma(inverse_dispersion) \\\n",
    "                        - logit \n",
    "\n",
    "        expr_zero = - tf.nn.softplus( - logit) \\\n",
    "                    + tf.nn.softplus(- logit + tf.log(inverse_dispersion + eps) * inverse_dispersion \\\n",
    "                                     - tf.log(inverse_dispersion + mean + eps) * inverse_dispersion) \n",
    "        \n",
    "        template = tf.cast(tf.less(x, eps), tf.float32)\n",
    "        expr =  tf.multiply(template, expr_zero) + tf.multiply(1 - template, expr_non_zero)\n",
    "        return tf.reduce_sum(expr, axis=-1)\n",
    "\n",
    "\n",
    "    def eval_cluster_on_test(self):\n",
    "\n",
    "        # Embedding points in the test data to the latent space\n",
    "        inp_encoder = self.data\n",
    "        labels = self.cell_type_data\n",
    "        batch_x_batch_info = self.batch_data_enc\n",
    "\n",
    "        latent_matrix = self.sess.run(self.z, feed_dict={self.x_input: inp_encoder, self.keep_prob: 1.0})\n",
    "        self.latent_matrix = latent_matrix\n",
    "        \n",
    "        \n",
    "        print(latent_matrix.shape)\n",
    "        print(labels.shape)\n",
    "\n",
    "        import scanpy as sc\n",
    "        Ann = sc.AnnData(inp_encoder)\n",
    "        Ann.obsm['final_embeddings'] = latent_matrix\n",
    "        Ann.obs['cell_type'] = labels.astype(str)\n",
    "        Ann.obs['batch'] = self.batch_data_non_enc.astype(str)\n",
    "\n",
    "        sc.pp.neighbors(Ann, use_rep='final_embeddings')  # use_rep = 'final_embeddings'\n",
    "        sc.tl.umap(Ann)\n",
    "        img = sc.pl.umap(Ann, color='cell_type', frameon=False)  # cells\n",
    "        print(img)\n",
    "        Ann.write(self.dataset_name)\n",
    "        \n",
    "        img2 = sc.pl.umap(Ann, color='batch', frameon=False)\n",
    "        # print(img2)\n",
    "        # img2.save('plot2.jpeg')\n",
    "\n",
    "        '''\n",
    "        self.Ann.obsm['final_embeddings'] = final_embedding\n",
    "        sc.pp.neighbors(self.Ann,use_rep='final_embeddings')\n",
    "        sc.tl.umap(self.Ann)\n",
    "        sc.pl.umap(self.Ann, color=C, frameon = False)\n",
    "        sc.pl.umap(self.Ann, color=B, frameon = False)\n",
    "        '''\n",
    "\n",
    "        K = np.size(np.unique(labels))\n",
    "        kmeans = KMeans(n_clusters=K, random_state=0).fit(latent_matrix)\n",
    "        y_pred = kmeans.labels_\n",
    "\n",
    "        print('Computing NMI ...')\n",
    "        NMI = nmi(labels.flatten(), y_pred.flatten())\n",
    "        print('Done !')\n",
    "\n",
    "        print('NMI = {}'.\n",
    "              format(NMI))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19891d9-3665-4baf-890d-7671d1df4b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "epoch = 200\n",
    "learning_rate = 0.0001\n",
    "beta1 = 0.5\n",
    "batch_size = 128\n",
    "z_dim = 10\n",
    "n_l = 2\n",
    "g_h_l1 = 256\n",
    "g_h_l2 = 256\n",
    "g_h_l3 = 0\n",
    "g_h_l4 = 0\n",
    "d_h_l1 = 256\n",
    "d_h_l2 = 256\n",
    "d_h_l3 = 0\n",
    "d_h_l4 = 0\n",
    "actv = \"sig\"\n",
    "leak = 0.2\n",
    "keep = 1.0\n",
    "trans = \"sparse\"\n",
    "checkpoint_dir = \"/data/eugene/AAE-20180306-Hemberg/test_checkpoint\"\n",
    "sample_dir = \"test_samples\"\n",
    "result_dir = \"test_result\"\n",
    "train = True\n",
    "g_iter = 2\n",
    "bn = False\n",
    "lam = 10.0\n",
    "sampler = \"normal\"\n",
    "model = \"dra\"\n",
    "X_dim = 2000\n",
    "datasets_folder = '/home/krushna/Documents/Data_integration/SCRNA_Datasets/All_h5ad/'\n",
    "dataset_name = \"Immune_Human\"\n",
    "\n",
    "'''\n",
    "dataset_names should be one of the following\n",
    "Immune_human\n",
    "Immune_human_mouse\n",
    "Lung\n",
    "Mouse_brain\n",
    "Pancreas\n",
    "Simulation1\n",
    "Simulation2\n",
    "'''\n",
    "batch_key_dic = {'Immune_Human' : 'batch',\n",
    "                 'Immune_human_mouse' : 'batch',\n",
    "                 'Lung' : 'batch',\n",
    "                 'Mouse_brain' : 'batch',\n",
    "                 'Pancreas' : 'tech',\n",
    "                 'Simulation1' : 'Batch',\n",
    "                 'Simulation2' : 'Batch'}\n",
    "cell_type_key_dic = {'Immune_Human' : 'final_annotation',\n",
    "                 'Immune_human_mouse' : 'final_annotation',\n",
    "                 'Lung' : 'cell_type',\n",
    "                 'Mouse_brain' : 'cell_type',\n",
    "                 'Pancreas' : 'celltype',\n",
    "                 'Simulation1' : 'Group',\n",
    "                 'Simulation2' : 'Group'}                   \n",
    "print(\"dataset: {}\".format(dataset_name))\n",
    "print(\"checkpoint_dir: {}\".format(checkpoint_dir))\n",
    "print(\"n_l: {}\".format(n_l))\n",
    "print(\"g_h_l1: {}\".format(g_h_l1))\n",
    "print(\"g_h_l2: {}\".format(g_h_l2))\n",
    "print(\"g_h_l3: {}\".format(g_h_l3))\n",
    "print(\"g_h_l4: {}\".format(g_h_l4))\n",
    "print(\"d_h_l1: {}\".format(d_h_l1))\n",
    "print(\"d_h_l2: {}\".format(d_h_l2))\n",
    "print(\"d_h_l3: {}\".format(d_h_l3))\n",
    "print(\"d_h_l4: {}\".format(d_h_l4))\n",
    "print(\"batch_size: {}\".format(batch_size))\n",
    "print(\"beta1: {}\".format(beta1))\n",
    "print(\"learning_rate: {}\".format(learning_rate))\n",
    "print(\"z_dim: {}\".format(z_dim))\n",
    "print(\"epoch: {}\".format(epoch))\n",
    "print(\"leak: {}\".format(leak))\n",
    "print(\"keep: {}\".format(keep))\n",
    "print(\"model: {}\".format(model))\n",
    "print(\"trans: {}\".format(trans))\n",
    "print(\"actv: {}\".format(actv))\n",
    "print(\"X_dim: {}\".format(X_dim))\n",
    "print(\"bn: {}\".format(bn))\n",
    "print(\"g_iter: {}\".format(g_iter))\n",
    "print(\"lam: {}\".format(lam))\n",
    "print(\"sampler: {}\".format(sampler))\n",
    "\n",
    "dataset_path = datasets_folder + dataset_name + '.h5ad'\n",
    "batch_key = batch_key_dic[dataset_name]\n",
    "cell_type_key = cell_type_key_dic[dataset_name]\n",
    "print('batch_key: ', batch_key)\n",
    "print('cell_type_key: ',cell_type_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5324c098-e540-4ce1-ba91-107ea06f1886",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "run_config = tf.ConfigProto()\n",
    "run_config.gpu_options.per_process_gpu_memory_fraction = 0.333\n",
    "run_config.gpu_options.allow_growth = True\n",
    "with tf.Session(config=run_config) as sess:\n",
    "\n",
    "    g_h_dim = [g_h_l1, g_h_l2, g_h_l3, g_h_l4]\n",
    "    d_h_dim = [d_h_l1, d_h_l2, d_h_l3, d_h_l4]\n",
    "\n",
    "    if model == 'dra':\n",
    "        test_dra = Test_DRA(\n",
    "            sess,\n",
    "            dataset_path,\n",
    "            batch_key,\n",
    "            cell_type_key,\n",
    "            epoch=epoch,\n",
    "            lr=learning_rate,\n",
    "            beta1=beta1,\n",
    "            batch_size=batch_size,\n",
    "            X_dim=X_dim,\n",
    "            z_dim=z_dim,\n",
    "            checkpoint_dir=checkpoint_dir,\n",
    "            sample_dir=sample_dir,\n",
    "            result_dir=result_dir,\n",
    "            num_layers=n_l,\n",
    "            g_h_dim=g_h_dim[:n_l],\n",
    "            d_h_dim=d_h_dim[:n_l],\n",
    "            gen_activation=actv,\n",
    "            leak=leak,\n",
    "            keep_param=keep,\n",
    "            trans=trans,\n",
    "            is_bn=bn,\n",
    "            g_iter=g_iter,\n",
    "            lam=lam,\n",
    "            sampler=sampler)\n",
    "\n",
    "    # show_all_variables()\n",
    "    if train:\n",
    "        if model == 'dra':\n",
    "            test_dra.train_cluster()\n",
    "    end = time.time()\n",
    "    print(\"Runtime of the program is\",end - start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8a058c-a958-4f81-baee-fcd7019df25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata = sc.read_h5ad(dataset_name+\".h5ad\")\n",
    "adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c687dc88-9847-4fd5-a7b0-eca49ccc56ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scIB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0d9705-f9ad-464c-bf11-cb491da09e7c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Trajectory is asking precomputed sudo time point\n",
    "results,ilisi_all,clisi_all,kbet_all = scIB.metrics.metrics(\n",
    "        adata,\n",
    "        adata,\n",
    "        batch_key = 'batch',\n",
    "        label_key = 'cell_type',\n",
    "        hvg_score_=False,\n",
    "        cluster_key='cluster',\n",
    "        cluster_nmi=None,\n",
    "        ari_=True,\n",
    "        nmi_=True,\n",
    "        nmi_method='arithmetic',\n",
    "        nmi_dir=None,\n",
    "        silhouette_=True,\n",
    "        embed='final_embeddings',\n",
    "        si_metric='euclidean',\n",
    "        pcr_=True,\n",
    "        cell_cycle_=False,\n",
    "        organism='mouse',\n",
    "        isolated_labels_=True,  # backwards compatibility\n",
    "        isolated_labels_f1_=True,\n",
    "        isolated_labels_asw_=True,\n",
    "        n_isolated=None,\n",
    "        graph_conn_=True,\n",
    "        kBET_=True,\n",
    "        kBET_sub=0.5,\n",
    "        lisi_graph_=True,\n",
    "        lisi_raw=True,\n",
    "        trajectory_=False,\n",
    "        type_=None,\n",
    "        verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2aac49-436d-4fd5-92cb-8b921174ccb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff297fe-205d-454d-a9bb-4cfec6b455ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.savetxt(dataset_name+\"_ilisi.csv\", ilisi_all, delimiter=\",\")\n",
    "np.savetxt(dataset_name+\"_clisi.csv\", clisi_all, delimiter=\",\")\n",
    "np.savetxt(dataset_name+\"_kbet_all.csv\",np.concatenate([np.array(val).reshape(1,-1) for val in kbet_all],axis = 0), delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40bba45-39f6-414c-a688-920073751689",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.dpi'] = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2607442f-6b2e-4884-b5a8-a6deee8fae03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.cluster import normalized_mutual_info_score as nmi\n",
    "\n",
    "\n",
    "latent_matrix = adata.obsm['final_embeddings'] \n",
    "labels = np.array(adata.obs['cell_type'] )\n",
    "K = np.size(np.unique(labels))\n",
    "kmeans = KMeans(n_clusters=K, random_state=0).fit(latent_matrix)\n",
    "y_pred = kmeans.labels_\n",
    "\n",
    "print('Computing NMI ...')\n",
    "NMI = nmi(labels.flatten(), y_pred.flatten())\n",
    "print('NMI = {}'.format(NMI))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0392d62-52a7-4546-8326-28e19a97de3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pp.neighbors(adata, use_rep='final_embeddings')  # use_rep = 'final_embeddings'\n",
    "sc.tl.umap(adata)\n",
    "sc.pl.umap(adata, color='cell_type', frameon=False)\n",
    "sc.pl.umap(adata, color='batch', frameon=False)\n",
    "sc.pl.umap(adata, color='cluster', frameon=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726f4547-7d11-4051-84be-0e8df2c7d736",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75822456-e816-458c-9df1-59ccf5efa1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trajectory is asking precomputed sudo time point\n",
    "results,ilisi_all,clisi_all,kbet_all = scIB.metrics.metrics(\n",
    "        adata,\n",
    "        adata,\n",
    "        batch_key = 'batch',\n",
    "        label_key = 'cell_type',\n",
    "        hvg_score_=False,\n",
    "        cluster_key='cluster',\n",
    "        cluster_nmi=None,\n",
    "        ari_=True,\n",
    "        nmi_=False,\n",
    "        nmi_method='arithmetic',\n",
    "        nmi_dir=None,\n",
    "        silhouette_=False,\n",
    "        embed='final_embeddings',\n",
    "        si_metric='euclidean',\n",
    "        pcr_=False,\n",
    "        cell_cycle_=False,\n",
    "        organism='mouse',\n",
    "        isolated_labels_=False,  # backwards compatibility\n",
    "        isolated_labels_f1_=False,\n",
    "        isolated_labels_asw_=False,\n",
    "        n_isolated=None,\n",
    "        graph_conn_=False,\n",
    "        kBET_=False,\n",
    "        kBET_sub=0.5,\n",
    "        lisi_graph_=False,\n",
    "        lisi_raw=False,\n",
    "        trajectory_=True,\n",
    "        type_=None,\n",
    "        verbose=False,\n",
    ")\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923d6baa-c93e-48a9-ab0f-ac97843e1159",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b7f088-57f8-42c3-a5e0-839cd7e01cab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
